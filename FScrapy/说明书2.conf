"""
1. 图片的爬取                      项目：《imgPro》
2. 图片懒加载 前端优化手段 影响爬虫    项目：《imgPro》
    src --> src2
3. 特殊的管道类ImagePileline的使用   项目：《imgPro》
4. 全站数据爬取类 CrawlSpider       项目：《sunCrawPro》
5. 分布式                         项目：《fbsPro》
6. 增量式                         项目：《zjsPro》
7. 反爬机制总结
"""

1.图片懒加载
    － 应用到标签的伪属性的时候

2.ImagePileline: 专门用作二进制数据下载和持久化存储的管道类
    － 使用流程：
            1. items.py 中定义字段
            2. 爬虫文件提交图片url
            3. 重写管道类 项目 imgPro/pipelines.py
            4. settings中指定图片存储的文件夹
                # 图片存储文件夹的名称+路径
                IMAGES_STORE = './imgLibs'
            5. 开启管道类

3.CrawlSpider
    - 一种基于scrapy进行全站数据爬取的一种新的技术手段

    - CrawlSpider就是Spider的一个子类
        - 连接提取器： LinkExtractor
        - 规则提取器： Rule

    - 使用流程： －－〉 项目：sunCrawPro
        1. 新建一个工程: scrapy startproject sunCrawPro
        2. cd 工程中:
        3. 新建一个爬虫文件: scrapy genspider -t crawl spiderName www.xxx.com
        4. settings中修改UA、robot协议、添加 日志等级

- 分布式   项目：《fbsPro》
    - 概念： 需要搭建一个分布式的机群，然后在机群的每一台电脑中执行同一组程序，让其对某一个网站的数据进行联合分布爬取。

    - 原生的scrapy框架是不可以实现分布式?
        － 原生的调度器不能共享
        － 管道不可以共享

    - 如何实现分布式？
        - scrapy+scrapy_redis实现分布式

    － scrapy_redis组件的作用是什么？
        －

    － 分布式的实现流程
       － 1. pip install scrapy-redis
       － 2. 创建工程
       － 3. cd 工程目录中
       － 4. 创建爬虫文件（a.创建基于Spider的爬虫文件 b.创建CrawSpider的爬虫文件）
       － 5. 修改爬虫类
            1. 爬虫文件导包：from scrapy_redis.spiders import RedisCrawlSpider
            2. 修改当前爬虫类的父类为RedisCrawSpider
            3. allow_domains和start_urls删除
            4. 添加一个新属性：redis_key = 'fbQueue'  # 表示的是可以被共享的调度器队列的名称
            5. 编写爬虫类的其它操作(常规操作)
       － 6. settings配置文件的配置
            1. UA 伪装
            2. Robots
            3. 管道的指定：
                ITEM_PIPELINES = {
                   'scrapy_redis.pipelines.RedisPipeline': 300,
                }
            4. 指定调度器
                # 增加了一个去重容器类的配置，作用使用Redis的set集合来存储请求的指纹数据，从而实现请求去重的持久化
                DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"
                SCHEDULER = "scrapy_redis.scheduler.Scheduler"
                # 配置调度器是否要持久化，也就是当爬虫结束了，要不要清空Redis中请求队列和去重指纹set。如果是True,就表示要持久化存储，就不清空数据，否则清空数据。
                SCHEDULER_PERSIST = True
            5. 指定redis数据库
                REDIS_HOST = 'redis服务的ip地址'
                REDIS_PORT = 6379
            6. redis配置文件的配置 redis.windows.conf:
                1. 关闭默认绑定：56Line: #bind 127.0.0.1
                2. 关闭保护模式：75Line: protected-mode no
            7. 启动redis的客户端和服务端<如果执行不了，关闭redis服务再试>
                1. 服务端的启动
                    redis-server.exe redis.windows.conf
                2. 客户端的启动
                    redis-cli
            8. 启动程序
                scrapy runspider xxx.py
            9. 向调度器的队列中扔一个起始的url:
                1. 队列是存在于redis中
                2. 开启redis的客户端并放入起始url
                    lpush fbsQueue http://wz.sun0768.com/index.php/question/questionType?type=4&page=

－ 增量式（应用场景不多）   项目：《zjsPro》
    － 概念： 用于监测数据更新的情况
    － 核心机制：去重。redis的set实现去重

    － 使用流程
        1.创建工程
            scrapy startproject zjsPro
        2.切换目录
            cd zjsPro
        3.创建全站数据爬取文件
            scrapy genspider -t crawl zjs www.xxx.com
        4.settings配置文件修改
            - UA设置
            - ROBOT协议
            - 日志等级
        5.爬虫文件
            1.注释通用域名
            2.填写起始url
            3.设置页码 规则提取器 rules
            4.爬取字段
                电影名称
                电影详情页detail_url
            5.导入redis数据库
                from redis import Redis
        6.items中编写要爬取的字段
        7.爬虫文件中导入items类
            提交item字段
        8.在管道文件pipelines中进行持久化存储
        9.在settings文件中开启管道
        10.执行脚本
            scrapy crawl zjs



** 反爬机制总结
    - robots
    - UA 伪装
    - 验证码
    - 代理
    - cookie
    - 动态变化的请求参数
    - js加密
    - js混淆
    - 图片懒加载
    - 动态数据的捕获
    - selenium: 规避检测




