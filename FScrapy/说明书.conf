例: huyaPro项目  firstBlood项目 <基础说明>
Scrapy框架使用
    -pySpider
-什么是框架?
    -就是一个具有很强通用性且集成了很多功能的项目模板(可以应用在各种需求中)
-scrapy集成好的功能:
    -高性能的数据解析操作(xpath)
    -高性能的数据下载
    -高性能的持久化存储
    -中间件
    -全栈数据爬取操作
    -分布式:redis
    -请求传参的机制(深度爬取)
    -scrapy中合理的应用selenium
-环境的安装
    1. pip install wheel
    2. 下载Twisted http://www.lfd.uci.edu/~gohlke/pythonlibs/#twisted
    3. 进入下载目录,执行 pip install Twisted-17.1.10-cp35m-win_amd64.whl
    4. pip install pywin32
    5. pip install scrapy

-创建工程
    - scrapy startproject ProName(工程名)
    - cd ProName
    - scrapy genspider spiderName(对应first.py) www.xxx.com: 创建爬虫文件
    - 执行:scrapy crawl spiderName  # 执行程序
    - settings:  # settings设置
        - 不遵从robots协议
        - 进行UA伪装
        - LOG_LEVEL = 'ERROR'  # 只显示error日志
        - LOG_FILE = 'log.txt'  # 通常不用,将日志生成到log.txt文件中
-scrapy的数据解析
    - extract(): 列表是有多个列表元素
    - extract_first(): 列表元素只有单个
-scrapy的持久化存储
    - 基于终端指令(有局限性):
        - 只可以将parse方法的返回值存储到磁盘文件中
        - scrapy crawl first -o file.txt  # 写入当前路径下的file.txt
    - 基于管道(万能的持久化存储): pipelines.py
        - 编码流程
            - 1.数据解析
            - 2.在item的类中定义相关的属性
            - 3.将解析的数据存储封闭到item类型的对象中 item['p']
            - 4.将item对象提交给管道
            - 5.在管道类中的process_item负责接收items对象,然后对item进行任意形式的存储
            - 6.在配置文件中开启管道
         - 细节补充:
            - 管道文件中的一个管道类表示将数据存储到某一种形式的平台中
            - 如果管道文件中定义了多个管道类,爬虫类提交的item会给到优先级最高的管道类
            - process_item方法的实现中的return item的操作表示将item传递给下一个即将被执行的管道类


简单使用流程:
    1.创建工程
        scrapy startproject ProName(工程名)
    2.切换目录
        cd ProName
    3.创建爬虫文件
        - scrapy genspider spiderName(对应first.py) www.xxx.com
    4.

    螃蟹在拨我的壳,笔记本在写我,漫天的我落在枫叶上雪花上,而你在想我!!!
