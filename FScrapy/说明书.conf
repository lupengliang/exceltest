"""
1. 基本使用  项目：《firstBlood》
2. 初步使用  项目：《huyaPro》
"""
例: 项目：《firstBlood》 项目：《huyaPro》
Scrapy框架使用
    -pySpider
-什么是框架?
    -就是一个具有很强通用性且集成了很多功能的项目模板(可以应用在各种需求中)
-scrapy集成好的功能:
    -高性能的数据解析操作(xpath)
    -高性能的数据下载
    -高性能的持久化存储
    -中间件
    -全站数据爬取操作
    -分布式:redis
    -请求传参的机制(深度爬取)
    -scrapy中合理的应用selenium
-环境的安装
    1. pip install wheel
    2. 下载Twisted http://www.lfd.uci.edu/~gohlke/pythonlibs/#twisted
    3. 进入下载目录,执行 pip install Twisted-17.1.10-cp35m-win_amd64.whl
    4. pip install pywin32
    5. pip install scrapy

-创建工程
    - scrapy startproject ProName(工程名)
    - cd ProName
    - scrapy genspider spiderName(对应first.py) www.xxx.com: 创建爬虫文件
    - 执行:scrapy crawl spiderName  # 执行程序
    - settings:  # settings设置
        - 不遵从robots协议
        - 进行UA伪装
        - LOG_LEVEL = 'ERROR'  # 只显示error日志
        - LOG_FILE = 'log.txt'  # 通常不用,将日志生成到log.txt文件中
-scrapy的数据解析
    - extract(): 列表是有多个列表元素
    - extract_first(): 列表元素只有单个
-scrapy的持久化存储
    - 基于终端指令(有局限性):
        - 只可以将parse方法的返回值存储到磁盘文件中
        - scrapy crawl first -o file.txt  # 写入当前路径下的file.txt
    - 基于管道(万能的持久化存储): pipelines.py
        - 编码流程
            - 1.数据解析
            - 2.在item的类中定义相关的属性
            - 3.将解析的数据存储封闭到item类型的对象中 item['p']
            - 4.将item对象提交给管道
            - 5.在管道类中的process_item负责接收items对象,然后对item进行任意形式的存储
            - 6.在配置文件中开启管道
         - 细节补充:
            - 管道文件中的一个管道类表示将数据存储到某一种形式的平台中
            - 如果管道文件中定义了多个管道类,爬虫类提交的item会给到优先级最高的管道类
            - process_item方法的实现中的return item的操作表示将item传递给下一个即将被执行的管道类


简单使用流程:
    1.创建工程
        scrapy startproject ProName(工程名)
    2.切换目录
        cd ProName
    3.创建爬虫文件
        - scrapy genspider spiderName(对应first.py) www.xxx.com
    4.settings里禁止robot协议
        ROBOTSTXT_OBEY = False
    5.settings里进行UA伪装
        USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36'
    6.settings里设置日志等级
        LOG_LEVEL = 'ERROR'
    7.spiderName禁用域名
        # allowed_domains = ['www.xxx.com']
    8.根据实际情况，开启下载中间件
         DOWNLOADER_MIDDLEWARES = {
            'wangyipeo.middlewares.WangyipeoDownloaderMiddleware': 543,
        }
    9.根据实际情况，开启下载管道进行持久化存储数据
        1. items文件中进行填写要存储的字段
        2. 在爬虫文件spiderName 导入items下的类
        3. 在爬虫文件spiderName 实例化items下的类 进行添加所需字段 向管道中 yield item 传递数据
        4. 在管道文件 pipelines 中进行存储数据
            mysql形式1：
                mysql 手动建库、建表
                    1. mysql -u root -p
                    2. use Spider;
                    3. create table wangyi (title varchar(200),content varchar(9999));
                    4. desc wangyi
                    5. 详情见 项目 wangyipeo/pipelines.py
            redis形式2：
                pass
    10. 在settings中开启管道
        ITEM_PIPELINES = {
           'wangyipeo.pipelines.WangyipeoPipeline': 300,
        }
    11. 在爬虫文件中关闭浏览器 项目 wangyipeo/wangyi.py
        def close(self，spider):pass

    螃蟹在拨我的壳,笔记本在写我,漫天的我落在枫叶上雪花上,而你在想我!!!
