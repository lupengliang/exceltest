"""
1. 持久化存储
2. selenium
3. 中间件
4. 回调函数，二次解析内部信息
5. 手动请求发送
"""
例: huyaAll项目
管道的持久化存储
    - 数据解析(爬虫类)
    - 将解析的数据封装到item类型的对象中(爬虫类)
    - 将item提交给管道: yield itme(爬虫类)
    - 在管道类的process_item中接收item对象并且进行任意形式的持久化存储操作(管道类)
    - 细节:
        - 将爬取的数据进行备份?
            - 一个管道类对应一种平台的持久化存储
        - 有多个管道类是否意味眷多个管道类都可以接受到爬虫文件提交的item?
            - 只有优先级最高的管道类才可以接受到item,剩下的管道类是需要从优先级最高的管道类中接收item
- 基于Spider父类进行全栈数据的爬取
    - 全站数据的爬取:将所有的页码对应的页面数据进行爬取
    - 手动请求的发送(get):
        scrapy.Request(url, callback)
    - 对yield的总结:
        - 向管道提交item的时候:yield item
        - 手动请求发送: yield scrapy.Request(url, callback)
    - 手动发起post请求(用的request模块比较多):
        yield scrapy.FormRequest(url, formdata, callback): fromdata是一个字典表示请求参数

- scrapy五大核心组件
    1.引擎(Scrapy)
        用来处理整个系统的数据流处理,触发事务(框架核心)
    2.爬虫(Spiders)
        爬虫主要干活的,用于从特定的网页中提取自己需要的信息,即所谓的实体(Item).用户也可以从中提取出链接,让Scrapy继续抓取下一个页面
    3.调度器(Scheduler)
        用来接受引擎发过来的请求,压入队列中,并在引擎再次请求的时候返回,可以想像成一个URL(抓取网页的网址或者说是
        链接)的优先队列,由它来决定下一个要抓取的网址是什么,同时去除重复的网址
    4.项目管道(Pipeline)
        负责处理爬虫从网页中抽取的实体,主要的功能是持久化实体、验证实体的有效性、清除不需要的信息。当页面被爬虫解析后，将被发送到项目管道，并
        经过几个特定的次序处理数据。
    5.下载器(Downloader)
        用于下载网页内容,并将网页内容返回给蜘蛛(Scrapy下载器是建立在twisted这个高效的异步模型上的)

- scrapy请求传参  例：爬取电影 moviePro
    - 作用：实现深度爬取
    - 使用场景：如果使用scrapy爬取的数据没有存在同一张页面中
    - 传递item: yield scrapy.Request(url, callback, meta)
    - 接收item: response.meta
- 提升scrapy爬取页面的效率？
    － 在配置文件中进行相关的配置即可：
        增加并发：
            默认scrapy开启的并发线程为32个，可以适当进行增加。在settings配置文件中修改CONCURRENT_REQUESTS = 100。

        降低日志级别：
            在运行scrapy时，会有大量日志信息的输出，为了减少cpu的使用率。可以设置log输出信息为INFO或者ERROR即可。在配置文件中编写：LOG_LEVEL = "INFO"。

        禁止cookie:
            如果不是真的需要cookie,则在scrapy爬取数据时可以禁止cookie从而减少CPU的使用率，提升爬取效率。在配置文件中编写：COOKIES_ENABLED = False

        禁止重试：
            对失败的HTTP进行重新请求（重试）会减慢爬取速度，因此可以重试。在配置文件中编写：RETRY_ENABLED = False

        减少下载超时：
            如果对一个非常慢的链接进行爬取，减少下载超时可以能让卡住的链接快速被放弃，从而提升效率。在配置文件中进行编写：DOWNLOAD_TIMEOUT = 10  超时时间为10s

- scrapy的中间件
    - 爬虫中间件（用的不太多）
    － 下载中间件（***）：处于引擎和下载器之间
       － 作用： 批量拦截所有的请求和响应
       － 为什么拦截请求  例：项目 middlePro
            － 篡改请求的头信息（UA伪装）
            － 修改请求对应的ip (代理)
       － 为什么拦截响应
            - 篡改响应数据，篡改响应对象

    － 爬取网易新闻的新闻标题和内容 wangyiPeo

    － selenium在scrapy中的使用流程
        － 在爬虫类中定义一个bro的属性，就是实例化的浏览器对象
        － 在爬虫类重写父类的一个closed(self, spider),在方法中关闭bro
        － 在中间件中进行浏览器自动化的操作